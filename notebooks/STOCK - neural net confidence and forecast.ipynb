{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_datareader as web\n",
    "import datetime\n",
    "\n",
    "# data from 2014 to 2018\n",
    "#start = datetime.datetime(2014, 9, 10)\n",
    "#end = datetime.datetime(2018,12,31)\n",
    "#df = web.DataReader('TSLA', 'yahoo', start, end) # Tesla stock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/Olatomiwa/Documents/SOL PLAATJE UNIVERSITY/HONOURS 2019/RESEARCH/From Supervisor Dr Mosia/first_yahoo_prices_volumes.csv', index_col = \"Date\", parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Adj Close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-09-10</th>\n",
       "      <td>19.680000</td>\n",
       "      <td>19.430000</td>\n",
       "      <td>19.650000</td>\n",
       "      <td>19.610001</td>\n",
       "      <td>4309400.0</td>\n",
       "      <td>18.881136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-09-11</th>\n",
       "      <td>19.540001</td>\n",
       "      <td>19.200001</td>\n",
       "      <td>19.469999</td>\n",
       "      <td>19.410000</td>\n",
       "      <td>6268000.0</td>\n",
       "      <td>18.688570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-09-12</th>\n",
       "      <td>19.530001</td>\n",
       "      <td>19.100000</td>\n",
       "      <td>19.530001</td>\n",
       "      <td>19.120001</td>\n",
       "      <td>6563400.0</td>\n",
       "      <td>18.409348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-09-15</th>\n",
       "      <td>19.209999</td>\n",
       "      <td>18.780001</td>\n",
       "      <td>19.180000</td>\n",
       "      <td>18.860001</td>\n",
       "      <td>7353800.0</td>\n",
       "      <td>18.159008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-09-16</th>\n",
       "      <td>19.240000</td>\n",
       "      <td>18.750000</td>\n",
       "      <td>18.809999</td>\n",
       "      <td>19.139999</td>\n",
       "      <td>5498400.0</td>\n",
       "      <td>18.428604</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 High        Low       Open      Close     Volume  Adj Close\n",
       "Date                                                                        \n",
       "2014-09-10  19.680000  19.430000  19.650000  19.610001  4309400.0  18.881136\n",
       "2014-09-11  19.540001  19.200001  19.469999  19.410000  6268000.0  18.688570\n",
       "2014-09-12  19.530001  19.100000  19.530001  19.120001  6563400.0  18.409348\n",
       "2014-09-15  19.209999  18.780001  19.180000  18.860001  7353800.0  18.159008\n",
       "2014-09-16  19.240000  18.750000  18.809999  19.139999  5498400.0  18.428604"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "High         False\n",
       "Low          False\n",
       "Open         False\n",
       "Close        False\n",
       "Volume       False\n",
       "Adj Close    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = df[['Volume', 'Adj Close']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the data for optimal performance\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler(feature_range = (0, 1))\n",
    "training_set_scaled = sc.fit_transform(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03474695, 0.00996007],\n",
       "       [0.05622704, 0.00925369],\n",
       "       [0.05946672, 0.00822942],\n",
       "       ...,\n",
       "       [0.16214805, 0.42137191],\n",
       "       [0.159868  , 0.43045983],\n",
       "       [0.115016  , 0.42991017]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set_scaled "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTMs expect our data to be in a specific format, usually a 3D array. We start by creating data in 60 timesteps and converting it into an array using NumPy. Next, we convert the data into a 3D dimension array with X_train samples, 60 timestamps, and one feature at each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(60, len(df)):\n",
    "    X_train.append(training_set_scaled[i-60:i, 0])\n",
    "    y_train.append(training_set_scaled[i, 0])\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\Olatomiwa\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Olatomiwa\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Olatomiwa\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Olatomiwa\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Olatomiwa\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Olatomiwa\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\Olatomiwa\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Olatomiwa\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Olatomiwa\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Olatomiwa\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Olatomiwa\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Olatomiwa\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add the LSTM layer and later add a few Dropout layers to prevent overfitting. We add the LSTM layer with the following arguments:\n",
    "\n",
    "+ 50 units which is the dimensionality of the output space\n",
    "\n",
    "+ return_sequences=True which determines whether to return the last output in the output sequence, or the full sequence\n",
    "\n",
    "+ input_shape as the shape of our training set.\n",
    "\n",
    "When defining the Dropout layers, we specify 0.2, meaning that 20% of the layers will be dropped. Thereafter, we add the Dense layer that specifies the output of 1 unit. After this, we compile our model using the popular adam optimizer and set the loss as the mean_squarred_error. This will compute the mean of the squared errors. Next, we fit the model to run on 100 epochs with a batch size of 32. Keep in mind that, depending on the specs of your computer, this might take a few minutes to finish running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1117 21:52:03.874985  6968 deprecation_wrapper.py:119] From C:\\Users\\Olatomiwa\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1117 21:52:04.425497  6968 deprecation_wrapper.py:119] From C:\\Users\\Olatomiwa\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1117 21:52:04.504652  6968 deprecation_wrapper.py:119] From C:\\Users\\Olatomiwa\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1117 21:52:05.286293  6968 deprecation_wrapper.py:119] From C:\\Users\\Olatomiwa\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W1117 21:52:05.297023  6968 deprecation.py:506] From C:\\Users\\Olatomiwa\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W1117 21:52:06.175885  6968 deprecation_wrapper.py:119] From C:\\Users\\Olatomiwa\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W1117 21:52:06.969022  6968 deprecation.py:323] From C:\\Users\\Olatomiwa\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W1117 21:52:10.213966  6968 deprecation_wrapper.py:119] From C:\\Users\\Olatomiwa\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1025/1025 [==============================] - 10s 10ms/step - loss: 0.0088\n",
      "Epoch 2/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0069\n",
      "Epoch 3/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0065\n",
      "Epoch 4/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0064\n",
      "Epoch 5/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0063\n",
      "Epoch 6/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0062\n",
      "Epoch 7/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0062\n",
      "Epoch 8/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0062\n",
      "Epoch 9/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0064\n",
      "Epoch 10/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0061\n",
      "Epoch 11/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0060\n",
      "Epoch 12/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0060\n",
      "Epoch 13/100\n",
      "1025/1025 [==============================] - 6s 5ms/step - loss: 0.0059\n",
      "Epoch 14/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0059\n",
      "Epoch 15/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0057\n",
      "Epoch 16/100\n",
      "1025/1025 [==============================] - 5s 5ms/step - loss: 0.0055\n",
      "Epoch 17/100\n",
      "1025/1025 [==============================] - 5s 5ms/step - loss: 0.0053\n",
      "Epoch 18/100\n",
      "1025/1025 [==============================] - 5s 5ms/step - loss: 0.0054\n",
      "Epoch 19/100\n",
      "1025/1025 [==============================] - 6s 5ms/step - loss: 0.0055\n",
      "Epoch 20/100\n",
      "1025/1025 [==============================] - 6s 5ms/step - loss: 0.0050\n",
      "Epoch 21/100\n",
      "1025/1025 [==============================] - 5s 5ms/step - loss: 0.0050\n",
      "Epoch 22/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0048\n",
      "Epoch 23/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0048\n",
      "Epoch 24/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0046\n",
      "Epoch 25/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0047\n",
      "Epoch 26/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0045\n",
      "Epoch 27/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0046\n",
      "Epoch 28/100\n",
      "1025/1025 [==============================] - 6s 5ms/step - loss: 0.0059\n",
      "Epoch 29/100\n",
      "1025/1025 [==============================] - 6s 5ms/step - loss: 0.0045\n",
      "Epoch 30/100\n",
      "1025/1025 [==============================] - 6s 5ms/step - loss: 0.0043\n",
      "Epoch 31/100\n",
      "1025/1025 [==============================] - 6s 5ms/step - loss: 0.0046\n",
      "Epoch 32/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0044\n",
      "Epoch 33/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0042\n",
      "Epoch 34/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0041\n",
      "Epoch 35/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0042\n",
      "Epoch 36/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0043\n",
      "Epoch 37/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0042\n",
      "Epoch 38/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0042\n",
      "Epoch 39/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0042\n",
      "Epoch 40/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0041\n",
      "Epoch 41/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0041\n",
      "Epoch 42/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0041\n",
      "Epoch 43/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0041\n",
      "Epoch 44/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0042\n",
      "Epoch 45/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0041\n",
      "Epoch 46/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0040\n",
      "Epoch 47/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0041\n",
      "Epoch 48/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0041\n",
      "Epoch 49/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0040\n",
      "Epoch 50/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0041\n",
      "Epoch 51/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0040\n",
      "Epoch 52/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0040\n",
      "Epoch 53/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0040\n",
      "Epoch 54/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0043\n",
      "Epoch 55/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0039\n",
      "Epoch 56/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0041\n",
      "Epoch 57/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0041\n",
      "Epoch 58/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0041\n",
      "Epoch 59/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0039\n",
      "Epoch 60/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0040\n",
      "Epoch 61/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0040\n",
      "Epoch 62/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0041\n",
      "Epoch 63/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0040\n",
      "Epoch 64/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0041\n",
      "Epoch 65/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0040\n",
      "Epoch 66/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0039\n",
      "Epoch 67/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0041\n",
      "Epoch 68/100\n",
      "1025/1025 [==============================] - 7s 6ms/step - loss: 0.0041\n",
      "Epoch 69/100\n",
      "1025/1025 [==============================] - 7s 6ms/step - loss: 0.0040A: 6s  - ETA: 3\n",
      "Epoch 70/100\n",
      "1025/1025 [==============================] - 7s 6ms/step - loss: 0.0041\n",
      "Epoch 71/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0043\n",
      "Epoch 72/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0043\n",
      "Epoch 73/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0040\n",
      "Epoch 74/100\n",
      "1025/1025 [==============================] - 7s 6ms/step - loss: 0.0040A: 1s - los\n",
      "Epoch 75/100\n",
      "1025/1025 [==============================] - 7s 6ms/step - loss: 0.0040\n",
      "Epoch 76/100\n",
      "1025/1025 [==============================] - 7s 7ms/step - loss: 0.0040\n",
      "Epoch 77/100\n",
      "1025/1025 [==============================] - 7s 6ms/step - loss: 0.0039\n",
      "Epoch 78/100\n",
      "1025/1025 [==============================] - 7s 7ms/step - loss: 0.0039\n",
      "Epoch 79/100\n",
      "1025/1025 [==============================] - 7s 7ms/step - loss: 0.0039\n",
      "Epoch 80/100\n",
      "1025/1025 [==============================] - 7s 7ms/step - loss: 0.0039\n",
      "Epoch 81/100\n",
      "1025/1025 [==============================] - 7s 7ms/step - loss: 0.0040\n",
      "Epoch 82/100\n",
      "1025/1025 [==============================] - 7s 6ms/step - loss: 0.0039\n",
      "Epoch 83/100\n",
      "1025/1025 [==============================] - 7s 6ms/step - loss: 0.0040\n",
      "Epoch 84/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0040\n",
      "Epoch 85/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0039\n",
      "Epoch 86/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0040\n",
      "Epoch 87/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0039\n",
      "Epoch 88/100\n",
      "1025/1025 [==============================] - 7s 7ms/step - loss: 0.0039\n",
      "Epoch 89/100\n",
      "1025/1025 [==============================] - 7s 7ms/step - loss: 0.0039\n",
      "Epoch 90/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0039\n",
      "Epoch 91/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0039\n",
      "Epoch 92/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0039\n",
      "Epoch 93/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0039\n",
      "Epoch 94/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0039\n",
      "Epoch 95/100\n",
      "1025/1025 [==============================] - 7s 7ms/step - loss: 0.0039\n",
      "Epoch 96/100\n",
      "1025/1025 [==============================] - 7s 7ms/step - loss: 0.0039\n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1025/1025 [==============================] - 7s 7ms/step - loss: 0.0038\n",
      "Epoch 98/100\n",
      "1025/1025 [==============================] - 7s 6ms/step - loss: 0.0040\n",
      "Epoch 99/100\n",
      "1025/1025 [==============================] - 7s 7ms/step - loss: 0.0041\n",
      "Epoch 100/100\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0040\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f0cb1509e8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor = Sequential()\n",
    "\n",
    "regressor.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "regressor.add(LSTM(units = 50))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "regressor.add(Dense(units = 1))\n",
    "\n",
    "regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "\n",
    "regressor.fit(X_train, y_train, epochs = 100, batch_size = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Future Stock using the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df.head(1025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = df.tail(60)\n",
    "real_stock_price = dataset_test.iloc[:, 5:6].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to predict future stock prices we need to do a couple of things after loading in the test set:\n",
    "\n",
    "+ Merge the training set and the test set on the 0 axis.\n",
    "+ Set the time step as 60 (as seen previously)\n",
    "+ Use MinMaxScaler to transform the new dataset\n",
    "+ Reshape the dataset as done previously\n",
    "\n",
    "After making the predictions we use inverse_transform to get back the stock prices in normal readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.30940000e+06, 1.88811359e+01],\n",
       "       [6.26800000e+06, 1.86885700e+01],\n",
       "       [6.56340000e+06, 1.84093475e+01],\n",
       "       ...,\n",
       "       [1.59261000e+07, 1.31034958e+02],\n",
       "       [1.57182000e+07, 1.33512390e+02],\n",
       "       [1.16285000e+07, 1.33362549e+02]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_total.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_total = pd.concat((dataset[['Volume', 'Adj Close']], dataset_test[['Volume', 'Adj Close']]), axis = 0)\n",
    "inputs = dataset_total[len(dataset_total) - len(dataset_test) - 60:].values\n",
    "#inputs = inputs.reshape(-1,1)\n",
    "inputs = sc.transform(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.08824758, 0.84637161],\n",
       "       [0.0822157 , 0.85990931],\n",
       "       [0.05520382, 0.85292102],\n",
       "       [0.04150815, 0.84882303],\n",
       "       [0.08677471, 0.86891008],\n",
       "       [0.06360788, 0.86162899],\n",
       "       [0.05212317, 0.86283645],\n",
       "       [0.0484492 , 0.85866536],\n",
       "       [0.07017276, 0.85325025],\n",
       "       [0.084714  , 0.85068913],\n",
       "       [0.07497853, 0.86225102],\n",
       "       [0.07662688, 0.87311772],\n",
       "       [0.06848274, 0.8627999 ],\n",
       "       [0.08079875, 0.8339316 ],\n",
       "       [0.06534835, 0.83660255],\n",
       "       [0.06522991, 0.84249326],\n",
       "       [0.06142214, 0.85767742],\n",
       "       [0.04623824, 0.86309253],\n",
       "       [0.04437932, 0.87015409],\n",
       "       [0.05825924, 0.88083793],\n",
       "       [0.05934717, 0.88621654],\n",
       "       [0.04340655, 0.87904504],\n",
       "       [0.05778327, 0.8729348 ],\n",
       "       [0.08997928, 0.87780103],\n",
       "       [0.11531211, 0.89722945],\n",
       "       [0.14956115, 0.88863113],\n",
       "       [0.20717224, 0.88263088],\n",
       "       [0.3009209 , 0.83645618],\n",
       "       [0.2217387 , 0.84750586],\n",
       "       [0.17227723, 0.86755636],\n",
       "       [0.19307847, 0.90231531],\n",
       "       [0.18508019, 0.91702393],\n",
       "       [0.13321365, 0.93670837],\n",
       "       [0.12615744, 0.95017291],\n",
       "       [0.11782466, 0.94461161],\n",
       "       [0.10266927, 0.96020662],\n",
       "       [0.08251729, 0.95771736],\n",
       "       [0.07154584, 0.96822393],\n",
       "       [0.09488595, 0.97927983],\n",
       "       [0.09527967, 0.9599506 ],\n",
       "       [0.0759886 , 0.93908366],\n",
       "       [0.06848274, 0.93593537],\n",
       "       [0.04784272, 0.94644205],\n",
       "       [0.05414111, 0.93937651],\n",
       "       [0.10239071, 0.92253665],\n",
       "       [0.07125631, 0.93403171],\n",
       "       [0.09357429, 0.95266542],\n",
       "       [0.04383645, 0.94351329],\n",
       "       [0.06960686, 0.93286019],\n",
       "       [0.04913684, 0.93637477],\n",
       "       [0.10554155, 0.91550783],\n",
       "       [0.10678741, 0.90514769],\n",
       "       [0.06806928, 0.91338453],\n",
       "       [0.06436132, 0.9233055 ],\n",
       "       [0.06111507, 0.91785077],\n",
       "       [0.04627663, 0.919608  ],\n",
       "       [0.18198528, 0.96946856],\n",
       "       [0.15803979, 1.        ],\n",
       "       [0.09275943, 0.98945694],\n",
       "       [0.07537335, 0.99037199],\n",
       "       [0.09474886, 0.96313538],\n",
       "       [0.1044591 , 0.92861359],\n",
       "       [0.09951734, 0.91364078],\n",
       "       [0.06247279, 0.91279893],\n",
       "       [0.17528002, 0.84013106],\n",
       "       [0.18638307, 0.8014726 ],\n",
       "       [0.15424957, 0.8432428 ],\n",
       "       [0.11079916, 0.80238782],\n",
       "       [0.09954476, 0.84064361],\n",
       "       [0.07787274, 0.83050305],\n",
       "       [0.13115951, 0.81758027],\n",
       "       [0.15572245, 0.779654  ],\n",
       "       [0.08861388, 0.78715871],\n",
       "       [0.15923959, 0.74996452],\n",
       "       [0.22993658, 0.67070733],\n",
       "       [0.24842486, 0.70156823],\n",
       "       [0.16975371, 0.6666071 ],\n",
       "       [0.19531575, 0.62022423],\n",
       "       [0.20879866, 0.68384975],\n",
       "       [0.19195873, 0.71251413],\n",
       "       [0.14281421, 0.73916504],\n",
       "       [0.11167652, 0.72748694],\n",
       "       [0.09148945, 0.71595533],\n",
       "       [0.06946758, 0.71335609],\n",
       "       [0.12013542, 0.72335016],\n",
       "       [0.12768625, 0.69479565],\n",
       "       [0.10078623, 0.69362418],\n",
       "       [0.15668426, 0.63457479],\n",
       "       [0.16425044, 0.67034121],\n",
       "       [0.13186141, 0.66258025],\n",
       "       [0.21798796, 0.68161663],\n",
       "       [0.52583676, 0.54265106],\n",
       "       [0.45298803, 0.47042258],\n",
       "       [0.4514011 , 0.4864571 ],\n",
       "       [0.26865251, 0.47045925],\n",
       "       [0.10043748, 0.47152084],\n",
       "       [0.21089337, 0.50099069],\n",
       "       [0.18984428, 0.50348   ],\n",
       "       [0.20806715, 0.52668976],\n",
       "       [0.13805561, 0.51734531],\n",
       "       [0.18751487, 0.53958881],\n",
       "       [0.23172311, 0.56381119],\n",
       "       [0.21014761, 0.51642914],\n",
       "       [0.17730015, 0.52075326],\n",
       "       [0.1743851 , 0.48161634],\n",
       "       [0.16007199, 0.49719053],\n",
       "       [0.17170804, 0.48374177],\n",
       "       [0.16683428, 0.48634353],\n",
       "       [0.11672795, 0.48630692],\n",
       "       [0.1168475 , 0.47736554],\n",
       "       [0.16922839, 0.46684839],\n",
       "       [0.14222309, 0.47916117],\n",
       "       [0.19184686, 0.44826932],\n",
       "       [0.19300499, 0.4357734 ],\n",
       "       [0.2243028 , 0.41550874],\n",
       "       [0.11465957, 0.40638407],\n",
       "       [0.17806565, 0.42844439],\n",
       "       [0.16214805, 0.42137191],\n",
       "       [0.159868  , 0.43045983],\n",
       "       [0.115016  , 0.42991017]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected lstm_1_input to have shape (60, 1) but got array with shape (60, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-96-86cc4b935440>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mpredicted_stock_price\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mpredicted_stock_price\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted_stock_price\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1147\u001b[0m                              'argument.')\n\u001b[0;32m   1148\u001b[0m         \u001b[1;31m# Validate user data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1149\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1150\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 751\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    752\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    136\u001b[0m                             \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m                             str(data_shape))\n\u001b[0m\u001b[0;32m    139\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected lstm_1_input to have shape (60, 1) but got array with shape (60, 2)"
     ]
    }
   ],
   "source": [
    "for i in range(60, 80):\n",
    "    X_test.append(inputs[i-60:i,])\n",
    "X_test = np.array(X_test)\n",
    "#X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "predicted_stock_price = regressor.predict(X_test)\n",
    "predicted_stock_price = sc.inverse_transform(predicted_stock_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method inverse_transform in module sklearn.preprocessing.data:\n",
      "\n",
      "inverse_transform(X) method of sklearn.preprocessing.data.MinMaxScaler instance\n",
      "    Undo the scaling of X according to feature_range.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    X : array-like, shape [n_samples, n_features]\n",
      "        Input data that will be transformed. It cannot be sparse.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sc.inverse_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEWCAYAAABliCz2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niI5IwY+I6Ih/ARLUyeGzBiSjAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(real_stock_price, color = 'black', label = 'Stock Price')\n",
    "plt.plot(predicted_stock_price, color = 'green', label = 'Predicted Stock Price')\n",
    "plt.title('Stock Price Prediction')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Stock Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot we can see that the real stock price went up while our model also predicted that the price of the stock will go up. This clearly shows how powerful LSTMs are for analyzing time series and sequential data.\n",
    "\n",
    "There are a couple of other techniques of predicting stock prices such as moving averages, linear regression, K-Nearest Neighbours, ARIMA and Prophet. These are techniques that one can test on their own and compare their performance with the Keras LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling mean\n",
    "df['Adj Close: 30 Day Mean'] = df['Adj Close'].rolling(window = 30).mean()\n",
    "df[['Adj Close', 'Adj Close: 30 Day Mean']].plot(figsize = 16, 16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling Normalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler(feature_range = (0, 1))\n",
    "training_set_scaled = sc.fit_transform(training_set)\n",
    "# Creating a data structure with 60 timesteps and 1 output\n",
    "X_train = []\n",
    "y_train = []\n",
    "for i in range(60, 1085):\n",
    "    X_train.append(training_set_scaled[i-60:i, 0])\n",
    "    y_train.append(training_set_scaled[i, 0])\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "\n",
    "# Reshaping\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1025/1025 [==============================] - 11s 11ms/step - loss: 0.0083\n",
      "Epoch 2/50\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0065\n",
      "Epoch 3/50\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0065\n",
      "Epoch 4/50\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0064\n",
      "Epoch 5/50\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0066\n",
      "Epoch 6/50\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0066\n",
      "Epoch 7/50\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0066\n",
      "Epoch 8/50\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0063\n",
      "Epoch 9/50\n",
      "1025/1025 [==============================] - 5s 5ms/step - loss: 0.0064\n",
      "Epoch 10/50\n",
      "1025/1025 [==============================] - 6s 5ms/step - loss: 0.0063\n",
      "Epoch 11/50\n",
      "1025/1025 [==============================] - 5s 5ms/step - loss: 0.0060\n",
      "Epoch 12/50\n",
      "1025/1025 [==============================] - 5s 5ms/step - loss: 0.0062\n",
      "Epoch 13/50\n",
      "1025/1025 [==============================] - 6s 5ms/step - loss: 0.0060\n",
      "Epoch 14/50\n",
      "1025/1025 [==============================] - 6s 5ms/step - loss: 0.0060\n",
      "Epoch 15/50\n",
      "1025/1025 [==============================] - 6s 5ms/step - loss: 0.0056\n",
      "Epoch 16/50\n",
      "1025/1025 [==============================] - 5s 5ms/step - loss: 0.0058\n",
      "Epoch 17/50\n",
      "1025/1025 [==============================] - 5s 5ms/step - loss: 0.0055\n",
      "Epoch 18/50\n",
      "1025/1025 [==============================] - 5s 5ms/step - loss: 0.0055\n",
      "Epoch 19/50\n",
      "1025/1025 [==============================] - 5s 5ms/step - loss: 0.0054\n",
      "Epoch 20/50\n",
      "1025/1025 [==============================] - 5s 5ms/step - loss: 0.0052\n",
      "Epoch 21/50\n",
      "1025/1025 [==============================] - 6s 5ms/step - loss: 0.0051\n",
      "Epoch 22/50\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0049\n",
      "Epoch 23/50\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0050\n",
      "Epoch 24/50\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0048\n",
      "Epoch 25/50\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0046\n",
      "Epoch 26/50\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0048\n",
      "Epoch 27/50\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0045\n",
      "Epoch 28/50\n",
      "1025/1025 [==============================] - 5s 5ms/step - loss: 0.0044\n",
      "Epoch 29/50\n",
      "1025/1025 [==============================] - 5s 5ms/step - loss: 0.0043\n",
      "Epoch 30/50\n",
      "1025/1025 [==============================] - 6s 5ms/step - loss: 0.0042\n",
      "Epoch 31/50\n",
      "1025/1025 [==============================] - 7s 7ms/step - loss: 0.0045\n",
      "Epoch 32/50\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0042\n",
      "Epoch 33/50\n",
      "1025/1025 [==============================] - 7s 7ms/step - loss: 0.0043\n",
      "Epoch 34/50\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0043\n",
      "Epoch 35/50\n",
      "1025/1025 [==============================] - 7s 7ms/step - loss: 0.0043\n",
      "Epoch 36/50\n",
      "1025/1025 [==============================] - 7s 7ms/step - loss: 0.0042\n",
      "Epoch 37/50\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0041\n",
      "Epoch 38/50\n",
      "1025/1025 [==============================] - 7s 6ms/step - loss: 0.0041\n",
      "Epoch 39/50\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0041\n",
      "Epoch 40/50\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0041\n",
      "Epoch 41/50\n",
      "1025/1025 [==============================] - 7s 7ms/step - loss: 0.0041\n",
      "Epoch 42/50\n",
      "1025/1025 [==============================] - 7s 7ms/step - loss: 0.0041\n",
      "Epoch 43/50\n",
      "1025/1025 [==============================] - 7s 6ms/step - loss: 0.0046\n",
      "Epoch 44/50\n",
      "1025/1025 [==============================] - 5s 5ms/step - loss: 0.0045\n",
      "Epoch 45/50\n",
      "1025/1025 [==============================] - 7s 7ms/step - loss: 0.0042\n",
      "Epoch 46/50\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0040\n",
      "Epoch 47/50\n",
      "1025/1025 [==============================] - 5s 5ms/step - loss: 0.0039\n",
      "Epoch 48/50\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0041\n",
      "Epoch 49/50\n",
      "1025/1025 [==============================] - 6s 6ms/step - loss: 0.0040\n",
      "Epoch 50/50\n",
      "1025/1025 [==============================] - 6s 5ms/step - loss: 0.0040\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14a56861c88>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialising the RNN\n",
    "regressor = Sequential()\n",
    "# Adding the first LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding a second LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding a third LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding a fourth LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 50))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding the output layer\n",
    "regressor.add(Dense(units = 1))\n",
    "# Compiling the RNN\n",
    "regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "\n",
    "# Fitting the RNN to the Training set\n",
    "regressor.fit(X_train, y_train, epochs = 50, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the real stock price of 2017\n",
    "dataset_test = pd.read_csv('Google_Stock_Price_Test.csv',index_col=\"Date\",parse_dates=True)\n",
    "real_stock_price = dataset_test.iloc[:, 1:2].values\n",
    "# Getting the predicted stock price\n",
    "dataset_total = pd.concat((dataset['Open'], dataset_test['Open']), axis = 0)\n",
    "inputs = dataset_total[len(dataset_total) - len(dataset_test) - 60:].values\n",
    "inputs = inputs.reshape(-1,1)\n",
    "inputs = sc.transform(inputs)\n",
    "X_test = []\n",
    "for i in range(60, 80):\n",
    "    X_test.append(inputs[i-60:i, 0])\n",
    "X_test = np.array(X_test)\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "predicted_stock_price = regressor.predict(X_test)\n",
    "predicted_stock_price = sc.inverse_transform(predicted_stock_price)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
